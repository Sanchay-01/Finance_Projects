\documentclass{article}

\usepackage{iclr2025_conference,times}

\usepackage{amsmath,amsfonts,bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

\title{NFL Outcome Prediction: Comparing Market Efficiency Across Moneyline and Spread Betting}

\author{Sanchay Bhutani \\
\texttt{your.email@university.edu} \\
\vspace{0.3cm} \\
\textbf{Final Project Report}
}

\begin{document}

\maketitle

\begin{abstract}
This research investigates the comparative predictability and market efficiency of the National Football League (NFL) across two distinct betting paradigms: the Moneyline market (predicting outright winners) and the Spread market (predicting outcomes relative to a handicapped point line). Historically, sports betting analytics has focused on single-target prediction, but this study employs a dual-target approach to demonstrate the internal efficiencies within the betting ecosystem. Using a comprehensive dataset of 9,236 games spanning from 1979 to 2024, we incorporate a variety of engineered features including rolling point differentials, weather conditions, and FiveThirtyEight's established quarterback-adjusted Elo ratings. We implement and compare a suite of machine learning models---including XGBoost, Random Forest, and Gaussian Naive Bayes---culminating in a Stacking Ensemble with a Logistic Regression meta-learner. To ensure scientific rigor and prevent temporal data leakage, we utilize a 5-fold TimeSeriesSplit validation strategy. Our results show a significant performance discrepancy: the Moneyline task achieves a robust ROC AUC of 0.6979, whereas the Spread task performs marginally above random chance with an AUC of 0.5278. Statistical validation via paired t-tests confirms that our advanced Stacking Ensemble provides no significant lift over a baseline Logistic Regression (p=0.134), suggesting that for high-entropy tasks like beating the spread, simple linear models capture the available signal as effectively as complex architectures. We conclude that while Moneyline outcomes are moderately predictable using historical trends, the NFL spread market exhibits a high degree of efficiency, effectively incorporating all public information into the line.
\end{abstract}

\section{Introduction}

The National Football League (NFL) represents the most-watched professional sports league in North America, with legal sports betting expanding to 38 US states as of 2024, generating over \$7 billion in annual handle \cite{mcdale2011}. In this high-stakes environment, the ability to accurately predict game outcomes drives not only betting strategies but also lineup optimization, draft analytics, and real-time win probability models used by broadcast networks to enhance viewer engagement. However, predicting NFL outcomes is notoriously difficult due to the "any given Sunday" phenomenon---a manifestation of the high inherent uncertainty caused by frequent injuries, volatile weather conditions, complex coaching adjustments, and the statistical noise inherent in a short 17-game regular season.

Traditional prediction systems often fail to capture the nuanced, high-dimensional features that define modern professional football. This project seeks to bridge the gap between traditional ratings and modern machine learning by investigating whether engineered performance features and ensemble architectures can provide a more accurate probabilistic view of game outcomes.

\subsection{Motivation}

The core motivation of this project is to test the limits of predictability in a highly efficient market. If a machine learning model can consistently outperform established baselines like FiveThirtyEight’s QB-ELO, it suggests that the market has not yet reached full informational efficiency. Conversely, if complex models fail to provide a significant edge over simple linear baselines, it serves as a powerful validation of the Efficient Market Hypothesis in sports analytics. This tension between model complexity and market efficiency informs our entire experimental design.

\subsection{Research Questions}

This project addresses three core questions that target both the predictive capability of our models and the underlying nature of the NFL betting market:
\begin{enumerate}
    \item \textbf{Market Efficiency:} How does predictive accuracy differ between the Moneyline market (predicting winners) and the more information-dense Spread market (predicting against-the-spread outcomes)?
    \item \textbf{Architechtural Utility:} Does a two-level Stacking Ensemble with a Logistic Regression meta-learner provide statistically significant improvements in discriminative accuracy compared to individual base learners?
    \item \textbf{Validation Rigor:} Can a temporal cross-validation strategy (TimeSeriesSplit) successfully prevent data leakage and provide a realistic estimate of real-world betting performance?
\end{enumerate}

\subsection{Contributions}

We make the following contributions:
\begin{itemize}
    \item Demonstrate significant performance gap between Moneyline (AUC 0.698) and Spread (AUC 0.520) prediction, providing empirical evidence of market efficiency
    \item Show that Stacking Ensembles provide no statistically significant improvement over Logistic Regression (paired t-tests: p=0.635, p=0.134), contrary to ensemble learning heuristics
    \item Provide rigorous temporal validation methodology using TimeSeriesSplit with 5 folds, preventing data leakage and ensuring realistic performance estimates
    \item Generate calibration curves demonstrating model reliability for both prediction tasks
\end{itemize}

\subsection{Applications}

Beyond sports betting, accurate probabilistic game predictions enable:
\begin{itemize}
    \item \textbf{Fantasy sports:} Optimal lineup construction under salary constraints
    \item \textbf{Broadcasting:} Real-time win probability graphics that update during games
    \item \textbf{Team analytics:} Quantifying impact of injuries, trades, or coaching changes
    \item \textbf{General ML research:} Sports prediction serves as accessible testbed for calibration, feature selection, and ensemble methods
\end{itemize}

\section{Problem Definition}

We formalize NFL game prediction as a binary classification problem with probabilistic output.

\subsection{Notation}

Let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ denote our dataset of $n=9{,}236$ games after preprocessing, where:
\begin{itemize}
    \item $x_i \in \mathbb{R}^{12}$ is a feature vector for game $i$ (before scaling and RFE)
    \item $y_i^{\text{ML}} \in \{0, 1\}$ is the Moneyline outcome (0 = away win, 1 = home win)
    \item $y_i^{\text{ATS}} \in \{0, 1\}$ is the Spread outcome (0 = favorite covers, 1 = underdog covers)
    \item Features include: spread, weather, home advantage, rolling point differentials, Elo ratings
\end{itemize}

We employ \textbf{TimeSeriesSplit} with 5 folds for cross-validation, ensuring no future data leaks into training.

\subsection{Objective Function}

Our goal is to learn a probabilistic classifier $f: \mathbb{R}^5 \rightarrow [0,1]$ that predicts:
\begin{equation}
\hat{p}_i = f(x_i) = P(Y=1 \mid X=x_i)
\end{equation}

We optimize for two complementary objectives:

\textbf{Discriminative Accuracy (ROC AUC):} Measures ability to rank games by outcome probability. AUC represents probability that a randomly chosen home-win game has higher predicted probability than a randomly chosen away-win game.

\textbf{Calibration Quality (Brier Score):} Measures mean squared error between predicted probabilities and actual outcomes:
\begin{equation}
\text{Brier}(f) = \frac{1}{n} \sum_{i=1}^n (\hat{p}_i - y_i)^2
\end{equation}

Lower Brier scores indicate better-calibrated probabilities—critical for betting where expected value depends directly on probability accuracy.

\subsection{Constraints and Betting Strategy}

We employ a confidence threshold strategy: only place bets when model confidence exceeds threshold $\tau$:
\begin{equation}
\text{Bet} = 
\begin{cases}
\text{Home win} & \text{if } \hat{p}_i \geq \tau_{\text{high}} \\
\text{Away win} & \text{if } \hat{p}_i \leq \tau_{\text{low}} \\
\text{No bet} & \text{otherwise}
\end{cases}
\end{equation}

We set $\tau_{\text{high}} = 0.6$ and $\tau_{\text{low}} = 0.4$ to filter uncertain predictions.

\subsection{Constraints and Modeling Restrictions}

We impose several critical constraints to ensure the validity and practical applicability of our results:

\begin{itemize}
    \item \textbf{Temporal Non-Leakage:} The primary constraint is that the training set $\mathcal{D}_{\text{train}}$ for any fold must contain only games that occurred chronologically before the games in the validation set $\mathcal{D}_{\text{val}}$. This is enforced via \texttt{TimeSeriesSplit}.
    \item \textbf{Informational Restriction:} No "in-game" data (e.g., halftime scores, player injuries sustained during the game) are used. Features are strictly derived from pre-kickoff information.
    \item \textbf{Market Condition:} We treat the closing spread as a static truth, although in reality, the line moves up to kickoff. Our model assumes the "Static Final Line" paradigm to maintain alignment with the provided dataset.
\end{itemize}

\subsection{Baseline Model}

We compare against FiveThirtyEight's QB-adjusted Elo probability (\texttt{qbelo\_prob1}) available in our dataset. Elo represents state-of-the-art in publicly available NFL predictions and has demonstrated strong performance over multiple seasons.

\section{Related Work}

\subsection{Statistical Models for Sports Prediction}

McHale and Morton \cite{mcdale2011} provide a comprehensive review of statistical approaches to sports forecasting, including regression-based models, time-series methods, and Bayesian networks. They emphasize the importance of probabilistic predictions over point predictions, as betting markets operate on odds rather than binary outcomes. \textbf{Relevance:} This work is foundational to our project as it justifies our focus on ROC AUC and Brier Score (probabilistic metrics) rather than simple accuracy, ensuring our model outputs are suitable for a betting context.

\subsection{Machine Learning in Sports Analytics}

Horvat and Job \cite{horvat2019} survey machine learning methods across multiple sports, comparing neural networks, SVMs, and ensembles. \textbf{Relevance and Comparison:} Our study complements this review by providing a deep dive into the NFL-specific subset of their survey. We differentiate our work by applying more rigorous statistical significance tests (paired t-tests) and explicit TimeSeriesSplit validation, which they identify as a common deficiency in the field.

Tsiligiridis and Ntakolia \cite{tsiligiridis2020} highlight feature engineering as a critical factor in sports data mining. \textbf{Relevance and Comparison:} We replicate their finding that domain-specific features outperform raw stats. We extend their research by testing these features specifically against the "hard" problem of point-spread prediction, whereas they primarily survey outright outcome modeling.

\subsection{XGBoost and Gradient Boosting}

Chen and Guestrin \cite{chen2016} introduce XGBoost, a scalable gradient boosting system with regularization to prevent overfitting. \textbf{Relevance:} We use XGBoost as a core base learner in our Stacking Ensemble. Their discussion of handling sparse data is particularly relevant to our dataset, which contains missing weather and Elo values for earlier seasons and neutral-site games.

\subsection{Feature Selection and Ensemble Methods}

Witten et al.\ \cite{witten2017} provide a foundational overview of Recursive Feature Elimination (RFE) and Stacking Ensemble architectures. Stacking (meta-learning) is described as an advanced way to combine models by training a meta-classifier on the predictions of base learners. \textbf{Relevance:} This is the central architectural reference for our methodology. We adopt their recommendation of using a simple linear model (Logistic Regression) as the meta-learner to avoid second-level overfitting, a strategy we test against baseline performance in our evaluation.

\section{Methodology}

\subsection{Data Collection}

We utilize two primary data sources:

\textbf{SpreadSpoke Scores:} Historical NFL game results from 1979-2025 (11,644 games), including:
\begin{itemize}
    \item Final scores (home/away points)
    \item Las Vegas betting lines (point spread, over/under)
    \item Game metadata (date, teams, neutral site indicator)
    \item Weather conditions (temperature, wind speed)
\end{itemize}

\textbf{FiveThirtyEight Elo Ratings:} Game-by-game Elo ratings containing:
\begin{itemize}
    \item Pre-game Elo ratings for both teams
    \item QB-adjusted Elo ratings accounting for quarterback value
    \item Elo-based win probabilities
\end{itemize}

\subsection{Data Preprocessing}

\textbf{Merge Challenges:} The two datasets required careful alignment:
\begin{itemize}
    \item \textbf{Team ID mismatches:} Raiders (LVR vs OAK), Washington (WAS vs WSH)
    \item \textbf{Date discrepancies:} Some games recorded on different dates (adjusted based on kickoff time)
    \item \textbf{Neutral site handling:} For Super Bowl and international games, home/away designation varies between sources. We performed bidirectional merge (home=team1, home=team2) and filled missing values.
\end{itemize}

\textbf{Missing Data:} After merge, 1,673 games (14.4\%) lacked Elo ratings, primarily early-season 1979 games. These rows were excluded from analysis.

\textbf{Target Variable:} Binary outcome defined as:
\begin{equation}
y = \mathbb{1}[\text{score\_home} > \text{score\_away}]
\end{equation}

Ties (extremely rare post-overtime rule changes) were excluded.

\subsection{Formal Algorithm: Rolling Point Differential}

A critical component of our feature engineering is the "Rolling Point Differential" feature, which serves as a proxy for a team's current momentum. Formally, for a team $T$ in week $w$ of season $s$, we define the rolling average as:

\begin{equation}
\mu_{\Delta, T, s, w} = \frac{1}{w-1} \sum_{i=1}^{w-1} (P_{T, s, i} - O_{T, s, i})
\end{equation}

where $P_{T, s, i}$ is the points scored by team $T$ in week $i$ and $O_{T, s, i}$ is the points scored by their opponent. For $w=1$, we initialize $\mu$ using the mean differential from season $s-1$, providing a "warm start" for the model. This algorithm addresses the limitation of traditional Elo ratings, which may be slow to adjust to sudden shifts in offensive or defensive performance mid-season.

\subsection{Formal Algorithm: Stacking Ensemble}

We implement a Stacking Ensemble (also known as Stacked Generalization) to combine the strengths of various base learners. The stacking procedure follows these formal steps:

\begin{enumerate}
    \item \textbf{Initialization:} Define $M$ base models $h_1, \dots, h_M$ (XGB, RF, K-NN, GNB, DT, LR) and a meta-learner $H$ (Logistic Regression).
    \item \textbf{Level-0 Training:} For each base model $h_m$, use $K$-fold cross-validation to generate "out-of-fold" predictions. Let $z_{i,m}$ be the predicted probability of model $m$ for game $x_i$ when game $i$ was in the validation fold.
    \item \textbf{Meta-Feature Construction:} Create a new training set $\mathcal{D}^* = \{(z_i, y_i)\}_{i=1}^n$, where the feature vector $z_i = [z_{i,1}, \dots, z_{i,M}]$ consists of the predictions from the base models.
    \item \textbf{Meta-Learner Training:} Train the meta-learner $H$ on $\mathcal{D}^*$ to learn the optimal combination of base model outputs:
    \begin{equation}
    \hat{y}_i = H(z_{i,1}, \dots, z_{i,M})
    \end{equation}
\end{enumerate}

By using cross-validated predictions ($z_{i,m}$), the meta-learner learns to weight models based on their performance on unseen data, effectively mitigating the risk of assigning high weights to base models that overfit the training set.

\subsection{Model Comparison}

We evaluated six classifiers via 5-fold cross-validation on training data:

\begin{table}[h]
\centering
\caption{Cross-Validation Performance (Training Data)}
\begin{tabular}{lc}
\toprule
Model & ROC AUC (Mean) \\
\midrule
Logistic Regression & 0.6962 \\
XGBoost & 0.6585 \\
Random Forest (100 trees) & 0.6796 \\
Decision Tree (depth=5, entropy) & 0.6935 \\
K-Nearest Neighbors & 0.6352 \\
Gaussian Naive Bayes & 0.6947 \\
\bottomrule
\end{tabular}
\end{table}

Logistic Regression and Decision Tree performed best, with XGBoost underperforming expectations possibly due to already-strong Elo features leaving limited room for complex interactions.

\subsection{Model Architecture}

We compare six base classifiers and one Stacking Ensemble:

\textbf{Base Models:}
\begin{itemize}
    \item Logistic Regression (solver='liblinear')
    \item K-Nearest Neighbors (default)
    \item Gaussian Naive Bayes
    \item XGBoost (random\_state=0)
    \item Random Forest (100 trees, random\_state=0)
    \item Decision Tree (max\_depth=5, criterion='entropy')
\end{itemize}

\textbf{Stacking Ensemble:}
Following \cite{witten2017}, we construct a two-level architecture:
\begin{itemize}
    \item \textbf{Level 0 (Base Learners):} Logistic Regression, K-NN, Naive Bayes, XGBoost, Random Forest
    \item \textbf{Level 1 (Meta-Learner):} Logistic Regression trained on base model predictions
\end{itemize}

StackingClassifier uses cross-validated predictions from base models as features for the meta-learner, reducing overfitting compared to simple averaging.

\textbf{Feature Preprocessing:} All features are scaled using StandardScaler before training:
\begin{equation}
x'_{ij} = \frac{x_{ij} - \mu_j}{\sigma_j}
\end{equation}
where $\mu_j, \sigma_j$ are mean and standard deviation of feature $j$ computed on training folds only.

\textbf{Rationale for Scaling:} Distance-based models (K-NN) and regularized models (Logistic Regression) are sensitive to feature scales. Weather features (temperature, wind) have different units than Elo ratings (1000-2000 range).

\section{Evaluation}

\subsection{Experimental Setup}

We train and evaluate all models on two prediction tasks:
\begin{itemize}
    \item \textbf{Task 1 (Moneyline):} Predict home team win (binary: home win=1, away win=0)
    \item \textbf{Task 2 (Spread):} Predict if home team covers spread (binary: cover=1, fail=0)
\end{itemize}

Each model is evaluated using 5-fold TimeSeriesSplit cross-validation. For the Stacking Ensemble, we perform paired t-tests comparing cross-validation scores against the best single model.

\subsection{Evaluation Metrics}

\textbf{ROC AUC (Receiver Operating Characteristic - Area Under Curve):} Measures discriminative ability. AUC=0.5 indicates random guessing, AUC=1.0 perfect separation.

\textbf{Brier Score:} Mean squared error of probabilities, $\text{Brier} = \frac{1}{n}\sum (\hat{p}_i - y_i)^2$. Lower is better (perfectly calibrated model has Brier $\leq 0.25$).

\textbf{Betting Win Percentage:} Among games where model confidence exceeds threshold ($\hat{p} \geq 0.6$ or $\leq 0.4$), fraction of correct predictions.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Cross-Validation Performance (5-Fold TimeSeriesSplit, 9,236 games)}
\begin{tabular}{lcc}
\toprule
Model & Moneyline AUC & Spread AUC \\
\midrule
Logistic Regression & \textbf{0.6979} & \textbf{0.5278} \\
K-Nearest Neighbors & 0.6255 & 0.5023 \\
Gaussian Naive Bayes & 0.6960 & 0.5261 \\
XGBoost & 0.6511 & 0.5097 \\
Random Forest & 0.6616 & 0.4993 \\
Decision Tree & 0.6869 & 0.5264 \\
\midrule
Stacking Ensemble & 0.6986 & 0.5196 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{model_comparison.png}
\caption{Model Performance Comparison: Moneyline (Win Prediction) vs. Spread (ATS Prediction). The dramatic performance gap illustrates the efficiency of the NFL spread market.}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{calibration_curve_result.png}
    \caption{Calibration Curve: Moneyline task. Note the strong proximity to the perfectly calibrated diagonal.}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{calibration_curve_spread_cover.png}
    \caption{Calibration Curve: Spread task. Represents reliability despite low discriminative power (AUC).}
\end{minipage}
\end{figure}

\textbf{Statistical Significance Tests:}\\
We performed paired t-tests comparing Stacking Ensemble vs. Logistic Regression (best single model):
\begin{itemize}
    \item \textbf{Moneyline}: $p = 0.635$ (NOT significant at $\alpha=0.05$)
    \item \textbf{Spread}: $p = 0.134$ (NOT significant at $\alpha=0.05$)
\end{itemize}

These results indicate that the Stacking Ensemble provides no statistically significant improvement over simple Logistic Regression for either task.

\subsection{Analysis}

\textbf{The Performance Gap and Market Efficiency:} The most striking finding of our experimentation is the dramatic discrepancy between the predictability of outright winners (Moneyline) and point-spread outcomes. With an AUC of nearly 0.70, our models demonstrate a strong ability to distinguish between teams that win and lose. However, in the Spread market, performance collapses to an AUC of 0.5278, which is only marginally better than a random coin flip. This provides robust empirical evidence for the \textbf{Efficient Market Hypothesis} as applied to NFL betting. The point spread acts as a sophisticated "information aggregator," incorporating weather, injuries, and public sentiment so effectively that the remaining signal is almost entirely noise.

\textbf{Ensemble Learning and the "No Free Lunch" Theorem:} Contrary to the common heuristic in machine learning that deep stacking ensembles improve generalization by reducing variance, our Stacking Ensemble showed no statistically significant improvement over the baseline Logistic Regression. The failure of the ensemble to extract additional predictive value suggests that we have hit a "predictability ceiling" defined by the data themselves, rather than the model architecture. In tasks with such high entropy and low signal-to-noise ratios, the limiting factor is the informational content of the features, not the complexity of the mapping function.

\textbf{Dominance of Linear Baselines:} The fact that simple Logistic Regression outperformed or matched sophisticated non-linear models like XGBoost and Random Forest is a critical highlight. This aligns with Occam's Razor in predictive modeling: when the relationship between features and target is weak or when the data are extremely noisy, simpler models often generalize better by avoiding the capture of spurious patterns. Logistic Regression essentially provides the most conservative "best guess" given the noisy inputs.

\textbf{Calibration Verification:} We generated calibration curves for our final models (using the calibrated parameters in the script) to ensure that the predicted probabilities $\hat{p}$ map accurately to observed frequencies. Our analysis confirmed high calibration quality, which is essential for any real-world betting strategy where the Expected Value ($EV$) of a bet depends linearly on the accuracy of the probability estimate.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Cross-validation only:} We report cross-validation results, not held-out test set performance. TimeSeriesSplit provides realistic estimates, but external validation on future seasons would strengthen claims.
    \item \textbf{NFL-specific:} Methodology not validated on other sports (NBA, MLB, soccer).
    \item \textbf{Limited features:} Did not incorporate injuries, detailed weather scraping, travel distance, or rest days.
    \item \textbf{No ROI calculation:} Absence of historical betting odds prevents true profitability assessment for betting strategies.
\end{enumerate}

\section{Conclusions}

This project provides empirical evidence for strong market efficiency in NFL betting, particularly in spread markets. Key findings:

\begin{itemize}
    \item \textbf{Spread markets are highly efficient:} AUC of 0.520 (barely above random 0.500) indicates that publicly available features cannot beat the spread consistently.
    \item \textbf{Moneyline prediction is easier but still limited:} AUC of 0.698 shows moderate predictive power for outright winners.
    \item \textbf{Stacking Ensembles provide no benefit:} Paired t-tests (p=0.635, p=0.134) demonstrate that complex ensembles cannot extract additional signal beyond Logistic Regression.
    \item \textbf{Simple models suffice:} Logistic Regression outperformed XGBoost and Random Forest on both tasks.
\end{itemize}

\subsection{Practical Implications}

For sports bettors, our results suggest that beating the spread market with publicly available data is nearly impossible—the market has already incorporated all such information into the line. For researchers, this demonstrates the importance of statistical rigor (TimeSeriesSplit, paired t-tests) to avoid over-optimistic conclusions about ensemble methods.

\subsection{Future Work}

Several promising research directions:
\begin{enumerate}
    \item \textbf{Deep learning:} LSTM networks to model sequential game outcomes within season, potentially capturing momentum effects more flexibly than rolling averages
    \item \textbf{Player-level modeling:} Incorporate injury reports, starter/backup quarterback differences, key defensive player absences
    \item \textbf{Live updating:} Real-time probability updates during game based on score, time remaining, field position
    \item \textbf{Multi-sport generalization:} Validate whether ensemble + calibration approach transfers to NBA, MLB, or international soccer
    \item \textbf{Market efficiency testing:} Compare model predictions to actual betting market odds to identify market inefficiencies
\end{enumerate}

\subsection{Reproducibility}

Complete code available in \texttt{fml.py} with comprehensive docstrings. All experiments reproducible on standard hardware (runtime $\sim$5 minutes).

\begin{thebibliography}{9}

\bibitem{mcdale2011}
I. G. McHale and D. E. Morton.
\textit{Predicting sports outcomes using statistical models.}
Journal of the Royal Statistical Society, 2011.

\bibitem{horvat2019}
T. Horvat and A. Job.
\textit{Machine learning methods for sports result prediction: A review.}
Information, 2019.

\bibitem{tsiligiridis2020}
T. Tsiligiridis and D. Ntakolia.
\textit{Data Mining and Machine Learning in Sports: A systematic review.}
Applied Sciences, 2020.

\bibitem{chen2016}
T. Chen and C. Guestrin.
\textit{XGBoost: A scalable tree boosting system.}
Proceedings of the 22nd ACM SIGKDD Conference, 2016.

\bibitem{witten2017}
I. Witten, E. Frank, M. Hall, and C. Pal.
\textit{Data Mining: Practical Machine Learning Tools and Techniques.}
Morgan Kaufmann, 2017.

\end{thebibliography}

\end{document}
